# Boosting LLM Performance with Automated Prompt Engineering

## Abstract
Prompt engineering is crucial for maximizing Large Language Model (LLM) performance, but manual optimization is time-consuming and often yields suboptimal results. This session benefits automated prompt engineering techniques through step-by-step optimization of an LLM application.
We begin by exploring the core purpose of automated prompt engineering, contrasting it with traditional prompt engineering, and discussing the benefits of automation. Building on this foundation, we then dive into a hands-on demonstration of improving the performance of an LLM pipeline through prompt engineering and few-shot learning. We review the metrics for automated prompt engineering and demonstrate performance improvement over traditional prompt engineering. The integrated GPU on an Intel AI PC achieves significant performance improvement without the need for a discrete GPU.
This session empowers developers to move beyond manual prompting by embracing automated techniques and open-source libraries to build high-performance LLM applications on inexpensive hardware.

## Benefits to the Ecosystem	
* Accelerates the adoption of open, powerful AI across enterprises.
Democratizes AI, making it more accessible to a wider range of developers.
* Provides a step-by-step guide to building and optimizing an LLM application.
* Empowers developers to move beyond manual prompting and embrace automated techniques for building high-performance LLM applications on accessible hardware.

## Status
Wiatlisted